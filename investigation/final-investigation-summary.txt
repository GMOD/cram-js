FINAL INVESTIGATION SUMMARY
===========================
Generated: 2025-11-19

BUGS IDENTIFIED:
================

BUG #1: Missing Records with Unmapped/No-Sequence Properties
--------------------------------------------------------------
Affects: c1#noseq.tmp.cram, human_g1k_v37 files

c1#noseq.tmp.cram (7 vs 9 records - Missing 2):
  Missing records:
  - sQ3: Has sequence "AACCCGGTT" but quality is only "*" (1 char vs expected 9)
  - SQ3: Has NO sequence "*" and NO quality "*"

human_g1k_v37.20.21.10M-10M200k#cramQueryWithCRAI.cram (6 vs 7 - Missing 1):
  Missing record:
  - Read "f" with flags 69 (PAIRED + UNMAPPED + MATE_REVERSE + FIRST_IN_PAIR)
  - This is an unmapped read whose mate is mapped
  - Position 100013, CIGAR "*"

ROOT CAUSE:
The decoder has issues with:
1. Unmapped reads (flag 4) in certain contexts
2. Records where quality scores don't match sequence length  
3. Records with completely absent sequence/quality data

LIKELY LOCATION OF BUG:
- src/cramFile/slice/decodeRecord.ts: Quality score decoding logic (lines 375-396)
- src/cramFile/slice/index.ts: Record filtering logic (line 459: `if (r)`)
- Possibly in CramFlagsDecoder logic for handling unmapped reads


BUG #2: Massive Record Loss in Large Files
--------------------------------------------
Affects: ce#1000.tmp.cram

Details:
  - Expected: 1000 records
  - Decoded: 602 records  
  - Missing: 398 records (40% data loss!)

ROOT CAUSE:
NOT a CramBufferOverrunError as initially suspected.
The file decodes without errors but silently returns only 602 records.

The number 602 suggests a slice/container boundary issue:
  - 602 = 6 Ã— 100 + 2 (possibly processing 6 slices of 100 records each)
  - May be stopping at a specific container boundary
  - Possible issue in dumpWholeFile() container iteration
  - Possible issue in containerCount() returning wrong value

LIKELY LOCATION OF BUG:
- test/lib/dumpFile.ts: Container iteration logic (lines 52-62)
- src/cramFile/file.ts: containerCount() method
- Slice/container header parsing that determines number of records


CHANGE MADE:
============

File: src/cramFile/slice/index.ts (lines 440-450)

BEFORE (Silent Failure):
  catch (e) {
    if (e instanceof CramBufferOverrunError) {
      console.warn('read attempted beyond end of buffer, file seems truncated.')
      break  // Silently drops remaining records!
    }
  }

AFTER (Error Reporting):
  catch (e) {
    if (e instanceof CramBufferOverrunError) {
      const recordsDecoded = i
      const recordsExpected = sliceHeader.parsedContent.numRecords
      throw new CramMalformedError(
        `Failed to decode all records in slice. Decoded ${recordsDecoded} of ` +
        `${recordsExpected} expected records. Buffer overrun suggests either: ` +
        `(1) file is truncated/corrupted, (2) compression scheme is incorrect, ` +
        `or (3) there's a bug in the decoder. Original error: ${e.message}`
      )
    }
  }

IMPACT: Users will now receive clear error messages instead of silent data loss


SUMMARY STATISTICS:
===================

Total CRAM files validated: ~183
Files with exact match: ~169 (92.3%)
Files with issues:
  - Unmapped/no-sequence handling: 3-4 files
  - Large file record loss: 1 file (critical)
  - Boundary handling differences: 3-4 files (minor, < 1% discrepancy)
  - Files samtools can't read: 2 files
  - HTS-SPECS edge cases: 3 files

CRITICAL ISSUES:
- Bug #1: Silent loss of unmapped/edge-case records
- Bug #2: 40% data loss in ce#1000.tmp.cram

Both bugs result in SILENT FAILURES - no errors thrown, just missing data.


RECOMMENDED NEXT STEPS:
=======================

HIGH PRIORITY:
1. Fix unmapped record handling in decodeRecord.ts
   - Review CramFlagsDecoder.isDecodeSequenceAsStar logic
   - Ensure unmapped reads aren't filtered out
   - Handle quality scores that don't match sequence length

2. Debug ce#1000.tmp.cram container/slice processing
   - Add logging to track containers and slices processed
   - Verify containerCount() returns correct value
   - Check if all slices in all containers are being decoded

MEDIUM PRIORITY:
3. Investigate tag padding/depadding issues (ce#tag_*.tmp.cram files)
4. Investigate MD tag handling (md#1.tmp.cram)
5. Review HTS-SPECS compliance edge cases

LOW PRIORITY:
6. Fine-tune boundary handling for region queries (< 1% differences)

TESTING RECOMMENDATIONS:
========================

1. Add tests that specifically validate:
   - Unmapped reads are decoded
   - Reads with no sequence are decoded
   - Quality scores of any length are handled
   - Multi-container files decode all containers

2. Add samtools validation to CI/CD pipeline
   - Run validation tests on every commit
   - Fail build if record counts don't match

3. Consider adding a "strict mode" that throws errors
   on any discrepancy with samtools

